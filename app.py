from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import os
from typing import List, Optional

app = Flask(__name__)

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
# =========================
MODEL_DIR = "./models_cache"
ALLAM_MODEL_PATH = os.path.join(
    MODEL_DIR,
    "models--humain-ai--ALLaM-7B-Instruct-preview/snapshots/a28dd1e67420cde72d3629c8633a974cf7d9c366"
)

# =========================
# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ù‡Ø§Ø²
# =========================
USE_CUDA = torch.cuda.is_available()
DEVICE = "cuda" if USE_CUDA else "cpu"

# bf16 Ù…Ù…ØªØ§Ø² Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ ÙƒØ±ÙˆØª NVIDIA Ø§Ù„Ø­Ø¯ÙŠØ«Ø©ØŒ Ù„Ùˆ Ù…Ø§ ÙŠØ¯Ø¹Ù…Ù‡ Ø®Ù„Ù‡ fp16
if USE_CUDA:
    try:
        _ = torch.tensor([1.0], device="cuda", dtype=torch.bfloat16)
        TORCH_DTYPE = torch.bfloat16
    except Exception:
        TORCH_DTYPE = torch.float16
else:
    TORCH_DTYPE = torch.float32

print(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Device: {DEVICE} | dtype: {TORCH_DTYPE}")

# =========================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# =========================
summarization_pipeline = None
tokenizer: Optional[AutoTokenizer] = None

print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        ALLAM_MODEL_PATH,
        trust_remote_code=True,
        local_files_only=True
    )

    # Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ù„Ø¨Ø¹Ø¶ Ù†Ù…Ø§Ø°Ø¬ LLaMA: pad = eos
    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # device_map Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† "auto" Ø£Ùˆ None (Ù…Ùˆ "cuda"/"cpu")
    if USE_CUDA:
        model = AutoModelForCausalLM.from_pretrained(
            ALLAM_MODEL_PATH,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=TORCH_DTYPE,
            device_map="auto",
            low_cpu_mem_usage=True
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            ALLAM_MODEL_PATH,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=TORCH_DTYPE,
            device_map=None,
            low_cpu_mem_usage=True
        )
        model.to("cpu")

    summarization_pipeline = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer
    )

    print("âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM Ø¨Ù†Ø¬Ø§Ø­")

except Exception as e:
    print(f"âœ— Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM: {str(e)}")
    summarization_pipeline = None


# =========================
# Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø§Ù„ØªÙˆÙƒÙ†/Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
# =========================
def _model_context_limit() -> int:
    """
    Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹Ø±ÙØ© Ø­Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ (context length) Ù…Ù† model/tokenizer.
    """
    try:
        cfg = summarization_pipeline.model.config
        if hasattr(cfg, "max_position_embeddings") and cfg.max_position_embeddings:
            return int(cfg.max_position_embeddings)
    except Exception:
        pass

    try:
        if tokenizer is not None and tokenizer.model_max_length and tokenizer.model_max_length < 10**9:
            return int(tokenizer.model_max_length)
    except Exception:
        pass

    # fallback Ø¢Ù…Ù†
    return 4096


def build_prompt(user_note: str, text: str) -> str:
    """
    ÙŠØ¨Ù†ÙŠ prompt Ø¨Ø´ÙƒÙ„ Chat Template Ø¥Ù† ÙƒØ§Ù† Ù…ØªÙˆÙØ±ØŒ ÙˆØ¥Ù„Ø§ Ù†Øµ Ø¹Ø§Ø¯ÙŠ.
    """
    system_prompt = (
        "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©.\n"
        "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ„Ø®ÙŠØµ:\n"
        "- Ø£Ø¹Ø·Ù†ÙŠ Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø¥Ø¶Ø§ÙØ§Øª\n"
        "- Ù„Ø§ ØªÙƒØªØ¨ \"Ø£Ø±Ø¬Ùˆ\"ØŒ \"ÙŠØ±Ø¬Ù‰\"ØŒ \"Ù…Ù„Ø§Ø­Ø¸Ø©\"ØŒ Ø£Ùˆ Ø£ÙŠ Ø¬Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠØ©\n"
        "- Ø§Ù„Ù…Ù„Ø®Øµ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹\n"
        "- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø·Ù„Ø¨ ØµÙŠØºØ© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø§Ù„ØªØ²Ù… Ø¨Ù‡Ø§ ØªÙ…Ø§Ù…Ø§Ù‹"
    )

    user_msg = (
        f"Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:\n{user_note}\n\n"
        f"Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡:\n{text}\n\n"
        "Ø§Ù„Ù…Ù„Ø®Øµ:"
    )

    # Ù„Ùˆ tokenizer ØªØ¯Ø¹Ù… chat template: Ù‡Ø°Ø§ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ù„Ù†Ù…Ø§Ø°Ø¬ Instruct
    if tokenizer is not None and hasattr(tokenizer, "apply_chat_template"):
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_msg},
        ]
        try:
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            return prompt
        except Exception:
            pass  # Ù†Ø±Ø¬Ø¹ Ù„Ù„Ùallback

    # fallback Ù†ØµÙ‘ÙŠ
    return f"{system_prompt}\n\n{user_msg}"


def split_text_by_tokens(text: str, token_budget: int) -> List[str]:
    """
    ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª (Ø¨Ø¯ÙˆÙ† special tokens).
    """
    if tokenizer is None:
        # fallback Ø¨Ø¯Ø§Ø¦ÙŠ (Ù†Ø§Ø¯Ø±Ù‹Ø§ Ù†Ø­ØªØ§Ø¬Ù‡)
        words = text.split()
        step = max(1, token_budget // 2)
        return [" ".join(words[i:i + step]) for i in range(0, len(words), step)]

    ids = tokenizer(text, add_special_tokens=False).input_ids
    chunks = []
    for i in range(0, len(ids), token_budget):
        chunk_ids = ids[i:i + token_budget]
        chunks.append(tokenizer.decode(chunk_ids, skip_special_tokens=True))
    return chunks


def generate_summary_once(text: str, note: str, max_new_tokens: int) -> str:
    """
    ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù„Ù‚Ø·Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©.
    """
    if summarization_pipeline is None or tokenizer is None:
        return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"

    prompt = build_prompt(note, text)

    # Ø¹Ø´Ø§Ù† Ù†Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Ø§ØªØ¬ Ø¨Ø¯ÙˆÙ† Ù„Ø¹Ø¨ "split('Ø§Ù„Ù…Ù„Ø®Øµ:')"
    prompt_len = len(prompt)

    with torch.inference_mode():
        out = summarization_pipeline(
            prompt,
            max_new_tokens=int(max_new_tokens),
            do_sample=True,
            temperature=0.3,
            top_p=0.9,
            num_return_sequences=1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    generated = out[0]["generated_text"]

    # Ù„Ùˆ Ø±Ø¬Ù‘Ø¹ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØŒ Ù†Ù‚ØµÙ‘Ù‡
    if isinstance(generated, str) and len(generated) >= prompt_len and generated[:prompt_len] == prompt:
        return generated[prompt_len:].strip()

    # fallback
    return generated.strip() if isinstance(generated, str) else str(generated)


def summarize_text(text: str, note: str, max_new_tokens: int = 150) -> str:
    """
    ØªÙ„Ø®ÙŠØµ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø³ÙŠØ§Ù‚.
    """
    if summarization_pipeline is None or tokenizer is None:
        return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"

    context_limit = _model_context_limit()

    # Ù†ØªØ±Ùƒ Ù‡Ø§Ù…Ø´ Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù†Ø¸Ø§Ù…/Ø§Ù„ÙŠÙˆØ²Ø± + ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    safety_margin = 512
    token_budget_for_text = max(256, context_limit - safety_margin - int(max_new_tokens))

    # Ù‚ÙŠØ§Ø³ ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ù†Øµ
    try:
        text_tokens = len(tokenizer(text, add_special_tokens=False).input_ids)
    except Exception:
        text_tokens = len(text.split())  # fallback

    if text_tokens <= token_budget_for_text:
        return generate_summary_once(text, note, max_new_tokens)

    # Ø¥Ø°Ø§ Ø·ÙˆÙŠÙ„: Ù‚Ø·Ù‘Ø¹ ÙˆÙÙ„Ø®Ù‘Øµ Ø£Ø¬Ø²Ø§Ø¡ Ø«Ù… Ù„Ø®Øµ Ù…Ù„Ø®ØµØ§Øª
    chunks = split_text_by_tokens(text, token_budget_for_text)
    summaries = []

    for i, chunk in enumerate(chunks, start=1):
        print(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø²Ø¡ {i}/{len(chunks)}...")
        try:
            part = generate_summary_once(chunk, note, max_new_tokens=max_new_tokens)
            if part:
                summaries.append(part)
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ {i}: {e}")

    if not summaries:
        return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡"

    merged = " ".join(summaries).strip()

    # Ù„Ùˆ Ø·Ù„Ø¹ Ø·ÙˆÙŠÙ„ØŒ Ø±Ø¬Ù‘Ø¹ ØªÙ„Ø®ÙŠØµ Ù†Ù‡Ø§Ø¦ÙŠ Ø£Ù‚ØµØ±
    final_max_new_tokens = max(80, int(max_new_tokens))
    try:
        return generate_summary_once(merged, "Ù„Ø®Øµ Ø§Ù„ØªØ§Ù„ÙŠ ÙƒÙ…Ù„Ø®Øµ Ù†Ù‡Ø§Ø¦ÙŠ Ù…ÙˆØ­Ø¯ ÙˆÙ…Ø®ØªØµØ± Ø¬Ø¯Ø§Ù‹", final_max_new_tokens)
    except Exception:
        return merged


# =========================
# API
# =========================
@app.route("/api/summarize", methods=["POST"])
def summarize_api():
    """
    Expected JSON:
    {
        "text": "...",
        "note": "...",
        "max_length": 150   # (Ù‡Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙƒÙ€ max_new_tokens)
    }
    """
    try:
        data = request.get_json(silent=True)
        if not data:
            return jsonify({"status": "error", "message": "Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª JSON"}), 400

        text = (data.get("text") or "").strip()
        note = (data.get("note") or "Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªØµØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø©").strip()

        # max_length Ø¹Ù†Ø¯Ùƒ Ù‡Ùˆ ÙØ¹Ù„ÙŠÙ‹Ø§ max_new_tokens (Ø¹Ø¯Ø¯ ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯)
        max_length = data.get("max_length", 150)
        try:
            max_length = int(max_length)
        except Exception:
            max_length = 150

        if not text:
            return jsonify({"status": "error", "message": "Ø§Ù„Ù†Øµ Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ÙØ§Ø±Øº"}), 400

        summary = summarize_text(text, note, max_new_tokens=max_length)

        return jsonify({
            "status": "success",
            "original_text": text,
            "note": note,
            "summary": summary,
            "text_length_chars": len(text),
            "summary_length_chars": len(summary),
            "device": DEVICE
        }), 200

    except Exception as e:
        return jsonify({"status": "error", "message": f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}"}), 500


@app.route("/health", methods=["GET"])
def health_check():
    return jsonify({
        "status": "healthy",
        "model_loaded": summarization_pipeline is not None,
        "device": DEVICE
    }), 200


@app.route("/", methods=["GET"])
def home():
    return jsonify({
        "message": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ API Ø§Ù„ØªÙ„Ø®ÙŠØµ",
        "endpoints": {
            "/api/summarize": "POST - Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ",
            "/health": "GET - ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"
        },
        "example": {
            "endpoint": "/api/summarize",
            "method": "POST",
            "body": {
                "text": "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡",
                "note": "Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ù…Ø®ØªØµØ±",
                "max_length": 150
            }
        }
    }), 200


if __name__ == "__main__":
    print("\n" + "=" * 50)
    print("ğŸš€ ØªØ´ØºÙŠÙ„ API Ø§Ù„ØªÙ„Ø®ÙŠØµ")
    print("=" * 50)
    print("ğŸ“ Ø§Ù„Ø±Ø§Ø¨Ø·: http://localhost:5001")
    print("ğŸ“ Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ: POST http://localhost:5001/api/summarize")
    print("ğŸ’š ÙØ­Øµ Ø§Ù„ØµØ­Ø©: GET http://localhost:5001/health")
    print("=" * 50 + "\n")

    app.run(
        host="0.0.0.0",
        port=5001,
        debug=True,
        use_reloader=False
    )
