from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, pipeline
import torch
import os

app = Flask(__name__)

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ©
MODEL_DIR = "./models_cache"
ALLAM_MODEL_PATH = os.path.join(MODEL_DIR, "models--humain-ai--ALLaM-7B-Instruct-preview/snapshots/a28dd1e67420cde72d3629c8633a974cf7d9c366")
ARABERT_MODEL_PATH = os.path.join(MODEL_DIR, "models--MostafaAhmed98--AraBert-Arabic-NER-CoNLLpp")

# Ø§Ø®ØªÙŠØ§Ø± device (GPU Ø¥Ø°Ø§ ØªÙˆÙØ±ØŒ ÙˆØ¥Ù„Ø§ CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Device: {device}")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM Ù„Ù„ØªÙ„Ø®ÙŠØµ
print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM...")
try:
    tokenizer = AutoTokenizer.from_pretrained(
        ALLAM_MODEL_PATH,
        trust_remote_code=True,
        local_files_only=True
    )
    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Llama Ù…Ø¨Ø§Ø´Ø±Ø©
    model = LlamaForCausalLM.from_pretrained(
        ALLAM_MODEL_PATH,
        local_files_only=True,
        torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
        device_map=device,
        low_cpu_mem_usage=True
    )
    summarization_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer
    )
    print("âœ“ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM Ø¨Ù†Ø¬Ø§Ø­")
except Exception as e:
    print(f"âœ— Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ALLaM: {str(e)}")
    summarization_pipeline = None


def create_summarization_prompt(user_prompt: str, text: str) -> str:
    """
    Ø¥Ù†Ø´Ø§Ø¡ prompt Ù…Ø­Ø³Ù‘Ù† Ù„Ù„ØªÙ„Ø®ÙŠØµ
    """
    # Prompt Ù…Ø­Ø³Ù‘Ù† ÙŠÙ…Ù†Ø¹ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
    system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ©.
Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙ„Ø®ÙŠØµ:
- Ø£Ø¹Ø·Ù†ÙŠ Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø¥Ø¶Ø§ÙØ§Øª
- Ù„Ø§ ØªÙƒØªØ¨ "Ø£Ø±Ø¬Ùˆ"ØŒ "ÙŠØ±Ø¬Ù‰"ØŒ "Ù…Ù„Ø§Ø­Ø¸Ø©"ØŒ Ø£Ùˆ Ø£ÙŠ Ø¬Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠØ©
- Ø§Ù„Ù…Ù„Ø®Øµ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø·Ù„Ø¨ ØµÙŠØºØ© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø§Ù„ØªØ²Ù… Ø¨Ù‡Ø§ ØªÙ…Ø§Ù…Ø§Ù‹"""
    
    full_prompt = f"""{system_prompt}

Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_prompt}

Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡:
{text}

Ø§Ù„Ù…Ù„Ø®Øµ:"""
    
    return full_prompt


def summarize_text(text: str, prompt: str, max_length: int = 150) -> str:
    """
    ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ ALLaM
    
    Args:
        text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡
        prompt: Ø£ÙˆØ§Ù…Ø±/ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªÙ„Ø®ÙŠØµ
        max_length: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ØªÙ„Ø®ÙŠØµ
    
    Returns:
        Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù„Ø®Øµ
    """
    if not summarization_pipeline:
        return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"
    
    # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ config.json)
    max_input_tokens = 3000  # Ù†ØªØ±Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¢Ù…Ù†
    
    # ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (ØªÙˆÙƒÙ† ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹)
    estimated_tokens = len(text.split())
    
    if estimated_tokens > max_input_tokens:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡
        words = text.split()
        chunk_size = max_input_tokens - 100  # Ø­Ø¬Ù… Ø§Ù„Ø¬Ø²Ø¡
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        summaries = []
        for i, chunk in enumerate(chunks):
            print(f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø²Ø¡ {i+1} Ù…Ù† {len(chunks)}...")
            
            full_prompt = create_summarization_prompt(prompt, chunk)
            
            try:
                result = summarization_pipeline(
                    full_prompt,
                    max_new_tokens=max_length,
                    num_return_sequences=1,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                generated_text = result[0]['generated_text']
                summary = generated_text.split("Ø§Ù„Ù…Ù„Ø®Øµ:")[-1].strip()
                summaries.append(summary)
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø²Ø¡ {i+1}: {str(e)}")
                continue
        
        # Ø¯Ù…Ø¬ Ù…Ù„Ø®ØµØ§Øª Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
        if summaries:
            final_summary = " ".join(summaries)
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ Ù…Ù„Ø®ØµÙ‡ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
            if len(final_summary.split()) > max_length:
                full_prompt = create_summarization_prompt(prompt, final_summary)
                try:
                    result = summarization_pipeline(
                        full_prompt,
                        max_new_tokens=max_length,
                        num_return_sequences=1,
                        temperature=0.3,
                        top_p=0.9,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                    final_summary = result[0]['generated_text'].split("Ø§Ù„Ù…Ù„Ø®Øµ:")[-1].strip()
                except:
                    pass
            
            return final_summary
        else:
            return "Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡"
    
    else:
        # Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø§Ø¯ÙŠØ©
        full_prompt = create_summarization_prompt(prompt, text)
        
        try:
            result = summarization_pipeline(
                full_prompt,
                max_new_tokens=max_length,
                num_return_sequences=1,
                temperature=0.3,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            
            generated_text = result[0]['generated_text']
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙˆÙ„Ø¯
            summary = generated_text.split("Ø§Ù„Ù…Ù„Ø®Øµ:")[-1].strip()
            return summary
        except Exception as e:
            return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ„Ø®ÙŠØµ: {str(e)}"


@app.route('/api/summarize', methods=['POST'])
def summarize_api():
    """
    API Endpoint Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ
    
    Expected JSON input:
    {
        "text": "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡",
        "note": "Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªÙ„Ø®ÙŠØµ - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„ØªÙ„Ø®ÙŠØµ Ù…Ø®ØªØµØ± ÙˆÙ…ÙÙŠØ¯",
        "max_length": 150  (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    }
    """
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±Ø³Ù„Ø©
        data = request.get_json()
        
        if not data:
            return jsonify({
                "status": "error",
                "message": "Ù„Ù… ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª JSON"
            }), 400
        
        text = data.get('text', '').strip()
        note = data.get('note', 'Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªØµØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø©').strip()
        max_length = data.get('max_length', 150)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Øµ
        if not text:
            return jsonify({
                "status": "error",
                "message": "Ø§Ù„Ù†Øµ Ù…ÙÙ‚ÙˆØ¯ Ø£Ùˆ ÙØ§Ø±Øº"
            }), 400
        
        if not note:
            note = "Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªØµØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø©"
        
        # ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ
        summary = summarize_text(text, note, max_length)
        
        return jsonify({
            "status": "success",
            "original_text": text,
            "note": note,
            "summary": summary,
            "text_length": len(text),
            "summary_length": len(summary)
        }), 200
    
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨: {str(e)}"
        }), 500


@app.route('/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"""
    return jsonify({
        "status": "healthy",
        "model_loaded": summarization_pipeline is not None,
        "device": device
    }), 200


@app.route('/', methods=['GET'])
def home():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return jsonify({
        "message": "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ API Ø§Ù„ØªÙ„Ø®ÙŠØµ",
        "endpoints": {
            "/api/summarize": "POST - Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØµÙˆØµ",
            "/health": "GET - ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"
        },
        "example": {
            "endpoint": "/api/summarize",
            "method": "POST",
            "body": {
                "text": "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªÙ„Ø®ÙŠØµÙ‡",
                "note": "Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ù…Ø®ØªØµØ±",
                "max_length": 150
            }
        }
    }), 200


if __name__ == '__main__':
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù…
    print("\n" + "="*50)
    print("ğŸš€ ØªØ´ØºÙŠÙ„ API Ø§Ù„ØªÙ„Ø®ÙŠØµ")
    print("="*50)
    print("ğŸ“ Ø§Ù„Ø±Ø§Ø¨Ø·: http://localhost:5001")
    print("ğŸ“ Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ: POST http://localhost:5001/api/summarize")
    print("ğŸ’š ÙØ­Øµ Ø§Ù„ØµØ­Ø©: GET http://localhost:5001/health")
    print("="*50 + "\n")
    
    app.run(
        host='0.0.0.0',
        port=5001,
        debug=True,
        use_reloader=False
    )
